{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multitasking model implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/Users/arpitha/Documents/295B/musicautobot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/arpitha/Documents/295B/musicautobot'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from musicautobot.numpy_encode import *\n",
    "from musicautobot.utils.file_processing import process_all, process_file\n",
    "from musicautobot.config import *\n",
    "from musicautobot.music_transformer import *\n",
    "from musicautobot.multitask_transformer import *\n",
    "from musicautobot.numpy_encode import stream2npenc_parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.text import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MultitaskTransformer Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pickling the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Location of your midi files\n",
    "midi_path = Path('data/midi/examples')\n",
    "midi_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Location to save dataset\n",
    "data_path = Path('data/numpy')\n",
    "data_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "data_save_name = 'musicitem_data_save.pkl'\n",
    "s2s_data_save_name = 'multiitem_data_save.pkl'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gather midi dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "midi_files = get_files(midi_path, '.mid', recurse=True); len(midi_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/arpitha/opt/anaconda3/lib/python3.8/site-packages/fastai/core.py:302: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return np.array(a, dtype=dtype, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "processors = [Midi2ItemProcessor()]\n",
    "data = MusicDataBunch.from_files(midi_files, data_path, processors=processors, \n",
    "                                 encode_position=True, dl_tfms=mask_lm_tfm_pitchdur, \n",
    "                                 bptt=5, bs=2)\n",
    "data.save(data_save_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'msk': {'x': tensor([[  4, 145,   4, 145,  74],\n",
       "          [  4, 145,   4, 145,   4]]),\n",
       "  'pos': tensor([[8, 8, 8, 8, 8],\n",
       "          [8, 8, 8, 8, 8]])},\n",
       " 'lm': {'x': tensor([[139,  64, 145,  61, 145],\n",
       "          [139,  64, 145,  61, 145]]),\n",
       "  'pos': tensor([[8, 8, 8, 8, 8],\n",
       "          [8, 8, 8, 8, 8]])}}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb, yb = data.one_batch(); xb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/arpitha/Documents/295B/musicautobot/musicautobot/multitask_transformer/transform.py:38: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  def to_idx(self): return np.array((self.melody.to_idx(), self.chords.to_idx()))\n"
     ]
    }
   ],
   "source": [
    "processors = [Midi2MultitrackProcessor()]\n",
    "s2s_data = MusicDataBunch.from_files(midi_files, data_path, processors=processors, \n",
    "                                     preloader_cls=S2SPreloader, list_cls=S2SItemList,\n",
    "                                     dl_tfms=melody_chord_tfm,\n",
    "                                     bptt=5, bs=2)\n",
    "s2s_data.save(s2s_data_save_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'c2m': {'enc': tensor([[  5,   1,  61, 145,  59],\n",
       "          [  5,   1,  69, 153,  66]]),\n",
       "  'enc_pos': tensor([[0, 0, 0, 0, 0],\n",
       "          [0, 0, 0, 0, 0]]),\n",
       "  'dec': tensor([[  6,   1,  85, 143,   8],\n",
       "          [  6,   1,  78, 139,   8]]),\n",
       "  'dec_pos': tensor([[0, 0, 0, 0, 0],\n",
       "          [0, 0, 0, 0, 0]])},\n",
       " 'm2c': {'enc': tensor([[  6,   1,  85, 143,   8],\n",
       "          [  6,   1,  78, 139,   8]]),\n",
       "  'enc_pos': tensor([[0, 0, 0, 0, 0],\n",
       "          [0, 0, 0, 0, 0]]),\n",
       "  'dec': tensor([[  5,   1,  61, 145,  59],\n",
       "          [  5,   1,  69, 153,  66]]),\n",
       "  'dec_pos': tensor([[0, 0, 0, 0, 0],\n",
       "          [0, 0, 0, 0, 0]])}}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb, yb = s2s_data.one_batch(); xb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "data = MusicDataBunch.empty(data_path)\n",
    "vocab = data.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_path = data_path/'pretrained/MultitaskSmallKeyC.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = multitask_model_learner(data, pretrained_path=pretrained_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose existing midi file as a starting point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('data/midi/examples/Levels - Avicii - Verse.mid'),\n",
       " PosixPath('data/midi/examples/Scary Monsters And Nice Sprites - Skrillex - Pre-Chorus.mid'),\n",
       " PosixPath('data/midi/examples/Can You Feel The Love Tonight - Elton John - Verse.mid'),\n",
       " PosixPath('data/midi/examples/Locked Out Of Heaven - Bruno Mars - Chorus.mid'),\n",
       " PosixPath('data/midi/examples/In The Hall Of The Mountain King - Edvard Grieg - Intro.mid')]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_dir = midi_path\n",
    "midi_files = get_files(example_dir, recurse=True, extensions='.mid'); midi_files[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "midi_files = get_files(example_dir, '.mid', recurse=True); len(midi_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('data/midi/examples/Levels - Avicii - Verse.mid')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = midi_files[0]; file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode file \n",
    "item = MusicItem.from_file(file, data.vocab)\n",
    "\n",
    "x = item.to_tensor()\n",
    "x_pos = item.get_pos_tensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# item.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                <div id='midiPlayerDiv2044802'></div>\n",
       "                <link rel=\"stylesheet\" href=\"//cuthbertLab.github.io/music21j/css/m21.css\"\n",
       "                    type=\"text/css\" />\n",
       "                <script>\n",
       "                require.config({\n",
       "                    paths: {'music21': '//cuthbertLab.github.io/music21j/src/music21'}\n",
       "                });\n",
       "                require(['music21'], function() {\n",
       "                               mp = new music21.miditools.MidiPlayer();\n",
       "                               mp.addPlayer('#midiPlayerDiv2044802');\n",
       "                               mp.base64Load('data:audio/midi;base64,TVRoZAAAAAYAAQADBABNVHJrAAAAGgD/UQMHRB4A/1kCAAAA/1gEBAIYCIgA/y8ATVRyawAAAYQA/wMFUGlhbm8AwAAA4ABAAMAAiACQTGSMAIBMAACQSmSQAIBKAACQSGSEAIBIAACQRWSUAIBFAACQQ2SEAIBDAACQQGSCAIBAAACQPmSCAIA+AACQPGSEAIA8AACQRWSEAIBFAACQRWSIAIBFAJAAkDxkhACAPAAAkDlkiACAOQCgAJBPZIQAgE8AAJBMZIQAgEwAAJBPZIQAgE8AAJBPZIQAgE8AAJBMZIQAgEwAAJBMZIQAgEwAAJBRZIQAgFEAAJBRZIIAgFEAAJBPZIQAgE8AAJBPZIIAgE8AAJBLZIIAgEsAAJBKZIQAgEoAggCQSmSCAIBKAACQSGSCAIBIAACQSmSEAIBKAACQRWSCAIBFAACQQ2SCAIBDAACQRWSIAIBFAACQPGSEAIA8AACQOWSIAIA5AACQQ2SEAIBDAACQQGSCAIBAAACQPmSCAIA+AACQPGSEAIA8AACQRWSEAIBFAACQRWSIAIBFAACQPGSEAIA8AACQOWSIAIA5AIgA/y8ATVRyawAAAh0A/wMFUGlhbm8AwAAA4ABAAMAAiACQLWQAkDJkAJA0ZJAAgC0AAIAyAACANAAAkDBkAJA0ZACQN2SQAIAwAACANAAAgDcAAJArZACQL2QAkDJkiACAKwAAgC8AAIAyAACQK2QAkDBkAJA0ZIQAgCsAAIAwAACANAAAkDVkAJA5ZACQPGSUAIA1AACAOQAAgDwAAJAtZACQMmQAkDRkkACALQAAgDIAAIA0AACQMGQAkDRkAJA3ZJAAgDAAAIA0AACANwAAkCtkAJAvZACQMmSIAIArAACALwAAgDIAAJArZACQMGQAkDRkhACAKwAAgDAAAIA0AACQNWQAkDlkAJA8ZACQQ2SUAIA1AACAOQAAgDwAAIBDAACQLWQAkDJkAJA0ZJAAgC0AAIAyAACANAAAkDBkAJA0ZACQN2SQAIAwAACANAAAgDcAAJArZACQL2QAkDJkiACAKwAAgC8AAIAyAACQK2QAkDBkAJA0ZIQAgCsAAIAwAACANAAAkDVkAJA5ZACQPGSUAIA1AACAOQAAgDwAAJAtZACQMmQAkDRkkACALQAAgDIAAIA0AACQMGQAkDRkAJA3ZJAAgDAAAIA0AACANwAAkCtkAJAvZACQMmSIAIArAACALwAAgDIAAJArZACQMGQAkDRkhACAKwAAgDAAAIA0AACQNWQAkDlkAJA8ZACQQ2SUAIA1AACAOQAAgDwAAIBDAIgA/y8A');\n",
       "                        });\n",
       "                </script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "item.play()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MultitaskTransformer trains on 3 separate tasks. \n",
    "1. NextWord\n",
    "2. Mask\n",
    "3. Sequence to Sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we train on 3 separate tasks, we can actually generate some really cool note sequences.\n",
    "\n",
    "1. NextWord/Autocomplete - Take a sequence of notes and predict the next note\n",
    " * 1a. Vanilla Language Model predictions - See [MusicTransformer](../music_transformer) project\n",
    "\n",
    "\n",
    "2. Mask/Remix - Mask certain parts of song and remix those portions.\n",
    " * 2a. Note Masking - Mask all the note pitches and create a new sequence with different notes, but same exact rhythm\n",
    " * 2b. Duration Masking - Mask the note durations. Generate a new sequence with the same melody, but with a different rhythm\n",
    "\n",
    "\n",
    "3. Seq2Seq/Translation - Generate melody from chords or vice versa. \n",
    " * 3a. New Melody - Generate a new melody from existing chords\n",
    " * 3b. Harmonization - Generate chords to acompany an existing melody"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. NextWord/Autocomplete\n",
    "\n",
    "Trim the song to only a few notes. Model will use these notes a seed and continue the idea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_len = 6 # 4 beats = 1 bar\n",
    "seed = item.trim_to_beat(seed_len)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
