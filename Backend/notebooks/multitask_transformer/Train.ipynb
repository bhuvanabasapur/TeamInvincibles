{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.chdir('../../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/arpitha/Documents/295B/musicautobot\n"
     ]
    }
   ],
   "source": [
    "%cd /Users/arpitha/Documents/295B/musicautobot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from musicautobot.numpy_encode import *\n",
    "from musicautobot.utils.file_processing import process_all, process_file\n",
    "from musicautobot.config import *\n",
    "from musicautobot.music_transformer import *\n",
    "from musicautobot.multitask_transformer import *\n",
    "from musicautobot.utils.stacked_dataloader import StackedDataBunch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.text import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MultitaskTransformer Training\n",
    "\n",
    "Multitask Training is an extension of [MusicTransformer](../music_transformer/Train.ipynb).\n",
    "\n",
    "Instead a basic language model that predicts the next word...\n",
    "\n",
    "We train on multiple tasks\n",
    "* [Next Word](../music_transformer/Train.ipynb)\n",
    "* [Bert Mask](https://arxiv.org/abs/1810.04805)\n",
    "* [Sequence to Sequence Translation](http://jalammar.github.io/illustrated-transformer/)\n",
    "\n",
    "This gives a more generalized model and also let's you do some really cool [predictions](Generate.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End to end training pipeline \n",
    "\n",
    "1. Create and encode dataset\n",
    "2. Initialize Transformer MOdel\n",
    "3. Train\n",
    "4. Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Location of your midi files\n",
    "midi_path = Path('data/midi/lmd_dataset')\n",
    "midi_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Location to save dataset\n",
    "data_path = Path('data/numpy')\n",
    "data_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "data_save_name = 'musicitem_data_save.pkl'\n",
    "s2s_data_save_name = 'multiitem_data_save.pkl'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Gather midi dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure all your midi data is in `musicautobot/data/midi` directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a pretty good dataset with lots of midi data:  \n",
    "https://www.reddit.com/r/datasets/comments/3akhxy/the_largest_midi_collection_on_the_internet/\n",
    "\n",
    "Download the folder and unzip it to `data/midi`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create dataset from MIDI files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "midi_files = get_files(midi_path, '.mid', recurse=True); len(midi_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2a. Create NextWord/Mask Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/arpitha/opt/anaconda3/lib/python3.8/site-packages/fastai/core.py:302: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return np.array(a, dtype=dtype, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "processors = [Midi2ItemProcessor()]\n",
    "data = MusicDataBunch.from_files(midi_files, data_path, processors=processors, \n",
    "                                 encode_position=True, dl_tfms=mask_lm_tfm_pitchdur, \n",
    "                                 bptt=5, bs=2)\n",
    "data.save(data_save_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'msk': {'x': tensor([[  8, 141,   4, 138,   8],\n",
       "          [  8, 141,   4, 138,   8]]),\n",
       "  'pos': tensor([[16, 16, 20, 20, 20],\n",
       "          [16, 16, 20, 20, 20]])},\n",
       " 'lm': {'x': tensor([[138,   8, 141,  50, 138],\n",
       "          [138,   8, 141,  50, 138]]),\n",
       "  'pos': tensor([[16, 16, 20, 20, 20],\n",
       "          [16, 16, 20, 20, 20]])}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb, yb = data.one_batch(); xb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key:\n",
    "* 'msk' = masked input\n",
    "* 'lm' = next word input\n",
    "* 'pos' = timestepped postional encoding. This is in addition to relative positional encoding\n",
    "\n",
    "Note: MultitaskTransformer trains on both the masked input ('msk') and next word input ('lm') at the same time.\n",
    "\n",
    "The encoder is trained on the 'msk' data, while the decoder is trained on 'lm' data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2b. Create sequence to sequence dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: only 1 track found. Inferring melody/chords\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/arpitha/Documents/295B/musicautobot/musicautobot/multitask_transformer/transform.py:38: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  def to_idx(self): return np.array((self.melody.to_idx(), self.chords.to_idx()))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: only 1 track found. Inferring melody/chords\n",
      "Could not extract melody and chords from midi file. Please make sure file contains exactly 2 tracks\n",
      "Could not extract melody and chords from midi file. Please make sure file contains exactly 2 tracks\n",
      "Warning: only 1 track found. Inferring melody/chords\n",
      "Could not extract melody and chords from midi file. Please make sure file contains exactly 2 tracks\n",
      "Warning: only 1 track found. Inferring melody/chords\n",
      "Could not extract melody and chords from midi file. Please make sure file contains exactly 2 tracks\n",
      "Warning: only 1 track found. Inferring melody/chords\n",
      "Could not extract melody and chords from midi file. Please make sure file contains exactly 2 tracks\n",
      "Could not extract melody and chords from midi file. Please make sure file contains exactly 2 tracks\n",
      "Could not extract melody and chords from midi file. Please make sure file contains exactly 2 tracks\n",
      "Could not extract melody and chords from midi file. Please make sure file contains exactly 2 tracks\n",
      "Could not extract melody and chords from midi file. Please make sure file contains exactly 2 tracks\n",
      "Could not extract melody and chords from midi file. Please make sure file contains exactly 2 tracks\n",
      "Warning: only 1 track found. Inferring melody/chords\n",
      "Could not extract melody and chords from midi file. Please make sure file contains exactly 2 tracks\n",
      "Could not extract melody and chords from midi file. Please make sure file contains exactly 2 tracks\n",
      "Warning: only 1 track found. Inferring melody/chords\n",
      "Could not extract melody and chords from midi file. Please make sure file contains exactly 2 tracks\n"
     ]
    }
   ],
   "source": [
    "processors = [Midi2MultitrackProcessor()]\n",
    "s2s_data = MusicDataBunch.from_files(midi_files, data_path, processors=processors, \n",
    "                                     preloader_cls=S2SPreloader, list_cls=S2SItemList,\n",
    "                                     dl_tfms=melody_chord_tfm,\n",
    "                                     bptt=5, bs=2)\n",
    "s2s_data.save(s2s_data_save_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'c2m': {'enc': tensor([[  5,   1,   8, 169, 107],\n",
       "          [  5,   1,   8, 155,  65]]),\n",
       "  'enc_pos': tensor([[ 0,  0,  0,  0, 32],\n",
       "          [ 0,  0,  0,  0, 18]]),\n",
       "  'dec': tensor([[  6,   1,   8, 153,  56],\n",
       "          [  6,   1,   8, 147,  70]]),\n",
       "  'dec_pos': tensor([[ 0,  0,  0,  0, 16],\n",
       "          [ 0,  0,  0,  0, 10]])},\n",
       " 'm2c': {'enc': tensor([[  6,   1,   8, 153,  56],\n",
       "          [  6,   1,   8, 147,  70]]),\n",
       "  'enc_pos': tensor([[ 0,  0,  0,  0, 16],\n",
       "          [ 0,  0,  0,  0, 10]]),\n",
       "  'dec': tensor([[  5,   1,   8, 169, 107],\n",
       "          [  5,   1,   8, 155,  65]]),\n",
       "  'dec_pos': tensor([[ 0,  0,  0,  0, 32],\n",
       "          [ 0,  0,  0,  0, 18]])}}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb, yb = s2s_data.one_batch(); xb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key:\n",
    "* 'c2m' = chord2melody translation\n",
    " * enc = chord\n",
    " * dec = melody\n",
    "* 'm2c' = next word input\n",
    " * enc = melody\n",
    " * dec = chord\n",
    "* 'pos' = timestepped postional encoding. Gives the model a better reference when translating\n",
    "\n",
    "Note: MultitaskTransformer trains both translations ('m2c' and 'c2m') at the same time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "batch_size = 2\n",
    "bptt = 128\n",
    "\n",
    "lm_data = load_data(data_path, data_save_name, \n",
    "                    bs=batch_size, bptt=bptt, encode_position=True,\n",
    "                    dl_tfms=mask_lm_tfm_pitchdur)\n",
    "\n",
    "s2s_data = load_data(data_path, s2s_data_save_name, \n",
    "                     bs=batch_size//2, bptt=bptt,\n",
    "                     preloader_cls=S2SPreloader, dl_tfms=melody_chord_tfm)\n",
    "\n",
    "# Combine both dataloaders so we can train multiple tasks at the same time\n",
    "data = StackedDataBunch([lm_data, s2s_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Model\n",
    "config = multitask_config(); config\n",
    "\n",
    "learn = multitask_model_learner(data, config.copy())\n",
    "# learn.to_fp16(dynamic=True) # Enable for mixed precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('example')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict\n",
    "\n",
    "---\n",
    "See [Generate.ipynb](Generate.ipynb) to use a pretrained model and generate better predictions\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# midi_files = get_files(midi_path, '.mid', recurse=True)\n",
    "midi_file = Path('data/midi/notebook_examples/single_bar_example.mid'); midi_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_word = nw_predict_from_midi(learn, midi_file, n_words=20, seed_len=8); next_word.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_melody = s2s_predict_from_midi(learn, midi_file, n_words=20, seed_len=4, pred_melody=True); pred_melody.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_notes = mask_predict_from_midi(learn, midi_file, predict_notes=True); pred_notes.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
